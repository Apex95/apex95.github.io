I"≈I<p>In this work I took a look at Tesseract 4.0‚Äôs performance at recognizing characters from a challenging dataset and proposed a minimalistic convolution-based approach for input image preprocessing that can boost the character-level <strong>accuracy</strong> from <strong>13.4%</strong> to <strong>61.6%</strong> (+359% relative change), and the <strong>F1 score</strong> from <strong>16.3%</strong> to <strong>72.9%</strong> (+347% relative change) on the aforementioned dataset. The convolution kernels are determined using reinforcement learning; moreover, to simulate the lack of ground truth in realistic scenarios, the training set consists of only 30 images while the testing set includes 10,000.</p>

<p>The dataset in cause is called <a href="https://pero.fit.vutbr.cz/brno_mobile_ocr_dataset" rel="nofollow">Brno</a>, and contains colored photographs of typed text, taken with handheld devices. Factors such as blurriness, low resolution, contrast, brightness are contributing to making the images challenging for an OCR engine.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/dataset-sample.png" alt="Resized image from the Brno dataset which contains text that was not recognized by Tesseract 4.0 during the evaluation (an empty string was returned)" />
  <figcaption><p>Resized image from the Brno dataset which contains text that was not recognized by Tesseract 4.0 during the evaluation (an empty string was returned)</p>
</figcaption>
</figure>

<p>During this experiment, the <em>out of the box</em> version of Tesseract 4.0 has been used, which implies:</p>
<ul>
  <li>no retraining of the OCR engine</li>
  <li>no lexicon / dictionary augmentations</li>
  <li>no hints about the language used in the dataset</li>
  <li>no hints about segmentation methods; automatic segmentation is used</li>
</ul>

<h2 id="problem-analysis">Problem Analysis</h2>

<p>Tesseract 4 has proven great performance when tested on favorable datasets by achieving good balance between precision and recall. It is assumed that this evaluation is performed on images that resemble scanned documents or book pages (with or without additional preprocessing) in which the number of distortions is minimal.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/tesseract-stats.png" alt="Tesseract 4's result when evaluated using the Google Books Dataset - results taken from [DAS 2016](https://github.com/tesseract-ocr/docs/tree/master/das_tutorial2016){:rel='nofollow'}" />
  <figcaption><p>Tesseract 4‚Äôs result when evaluated using the Google Books Dataset - results taken from <a href="https://github.com/tesseract-ocr/docs/tree/master/das_tutorial2016" rel="nofollow">DAS 2016</a></p>
</figcaption>
</figure>

<p>A high precision indicates favorable <em>True-Positives</em> to <em>False-Positives</em> ratio thus revealing proper differentiation between characters (i.e. a relatively small number of misclassifications).</p>

<p>However, it was pointed out in a previous article that <a href="https://codingvision.net/ai/evaluating-the-robustness-of-ocr-systems">Tesseract is not robust to noise</a>; certain <em>salt-and-pepper</em> noise patterns disrupt the character recognition process, leading to large segments of text being completely ignored by the OCR engine. This suggests a weakness in the segmentation methodology.</p>

<p>The existence of similar behavior given photographs of text which present more natural distortions is questioned, hence the experiment.</p>

<h2 id="challenges">Challenges</h2>

<p>Now, let‚Äôs focus on the different constraints and challenges:</p>

<h3 id="1-complex--closed-source-architecture">1. Complex / closed-source architecture</h3>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/tess-pipeline.png" alt="Tesseract's pipeline as [presented at DAS 2016](https://github.com/tesseract-ocr/docs/blob/master/das_tutorial2016/2ArchitectureAndDataStructures.pdf){:rel='nofollow'}" />
  <figcaption><p>Tesseract‚Äôs pipeline as <a href="https://github.com/tesseract-ocr/docs/blob/master/das_tutorial2016/2ArchitectureAndDataStructures.pdf" rel="nofollow">presented at DAS 2016</a></p>
</figcaption>
</figure>

<p>Modern OCR systems are more complex than basic convolutional neural networks as they need to perform multiple actions (e.g.: deskewing, layout detection, text rows segmentation), therefore finding ways to correctly compute gradients is a daunting task. Moreover, many of them do not provide access to source code thus making it difficult to use techniques such as <strong>FGSM</strong> or <strong>GAN</strong>s.</p>

<h3 id="2-binarization">2. Binarization</h3>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/binarization.png" alt="Result of the binarization procedure, using an adaptive threshold" />
  <figcaption><p>Result of the binarization procedure, using an adaptive threshold</p>
</figcaption>
</figure>

<p>An OCR system usually applies a binarization procedure (e.g.: <strong>Otsu</strong>‚Äôs method) to the image before running it through the main classifier in order to separate the text from the background, the ideal output being pure black text on a clean white background.</p>

<p>This proves troublesome because it restricts the samples generator from altering pixels using small values: as an example, converting a black pixel to a grayish color will be reverted in the binarization process thus generating no feedback.</p>

<h3 id="3-adaptive-classification">3. Adaptive classification</h3>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/adaptive-classifier.png" alt="Tesseract's adaptive classifier incorrectly recognizes an 'h' as a 'b', in the first image. In the second sample, Tesseract observes a correct 'h' character (confidence is larger than a threshold) adjusts the classifier's configuration and correctly classifies the first 'h'" />
  <figcaption><p>Tesseract‚Äôs adaptive classifier incorrectly recognizes an ‚Äòh‚Äô as a ‚Äòb‚Äô, in the first image. In the second sample, Tesseract observes a correct ‚Äòh‚Äô character (confidence is larger than a threshold) adjusts the classifier‚Äôs configuration and correctly classifies the first ‚Äòh‚Äô</p>
</figcaption>
</figure>

<p>This is specific to Tesseract, which is rather deprecated nowadays - still very popular, though. Modern classifiers might be using this method, too. It consists of performing 2 iterations over the same input image. In the first pass, characters which can be recognized with a certain confidence are selected and used as temporary training data. In the second pass, the OCR attempts to classify characters which were not recognized in the first iteration, but using what it previously learned.</p>

<p>Considering this, having an adversarial generator which alters one character at a time might not work as expected since that character might appear later in the image.</p>

<h3 id="4-lower-entropy">4. Lower entropy</h3>

<p>This refers to the fact that the input data is rather ‚Äòlimited‚Äô for an OCR system when compared to‚Ä¶ let‚Äôs say object recognition. As an example, images which contain 3D objects have larger variance than those which contain characters since the characters have a rather fixed shape and format. This should make it more difficult to create adversarial samples for character classifiers without applying distortions.</p>

<p>A direct consequence is that it greatly restricts the amount of noise that can be added to an image so that the readability is preserved.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/noise-readability.png" alt="Applying noise in an image usually decreases readability, which is not what we want here" />
  <figcaption><p>Applying noise in an image usually decreases readability, which is not what we want here</p>
</figcaption>
</figure>

<h3 id="5-dictionaries">5. Dictionaries</h3>

<p>OCR systems will attempt to improve their accuracy by employing dictionaries with predefined words. Altering a single character in a word (i.e.: the incremental approach) might not be effective in this case.</p>

<h2 id="targeted-ocr-systems">Targeted OCR Systems</h2>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/tesseract-gocr.png" alt="Tested locally on Tesseract 4.0 and remotely on Google's Cloud Vision OCR" />
  <figcaption><p>Tested locally on Tesseract 4.0 and remotely on Google‚Äôs Cloud Vision OCR</p>
</figcaption>
</figure>

<p>For this project, I used <strong>Tesseract 4.0</strong> for prototyping and testing, as it had no timing restrictions and allowed me to run a fast, parallel model with high throughput so I could test if the implementation works as expected. Later, I moved to <strong>Google‚Äôs Cloud Vision OCR</strong> and tried some ‚Äòremote‚Äô fuzzing through the API.</p>

<h2 id="methodology">Methodology</h2>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/architecture.png" alt="A rather simplified view of the flow; a feedback-based adversarial samples generator (in image: obfuscator) alters inputs in order to maximize the error of the OCR system" />
  <figcaption><p>A rather simplified view of the flow; a feedback-based adversarial samples generator (in image: obfuscator) alters inputs in order to maximize the error of the OCR system</p>
</figcaption>
</figure>

<p>In order to be able to cover even black box cases, I used a <strong>genetic algorithm</strong> guided by the feedback of the targeted OCR system. We observe that the confidence of the classifier, alone, is not a good metric for this problem, a score function based on the <a href="https://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">Levenshtein distance</a> and the <strong>amount of noise</strong> is employed.</p>

<p>One of the main problems here was the size of the search space which was partially solved by identifying regions of interest in the image and focusing only on these. Also, lots of parameter tuning‚Ä¶</p>

<h2 id="noise-properties">Noise properties</h2>

<p>Given the constraints, the following properties of the noise model must be matched:</p>

<ul>
  <li><strong>high contrast</strong> ‚Äì so it bypasses the binarization process and generates feedback</li>
  <li><strong>low density</strong> ‚Äì in order to maintain readability by exploiting the natural <strong>low-filtering</strong> capability of the human vision</li>
</ul>

<p>Applying <strong>salt-and-pepper</strong> noise in a smart manner will, hopefully, satisfy the constraints.</p>

<h2 id="working-modes">Working modes</h2>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/modes.png" alt="Different working modes for small and large characters, in order to preserve readability. Both managed to entirely hide the given text when tested on Tesseract 4.0" />
  <figcaption><p>Different working modes for small and large characters, in order to preserve readability. Both managed to entirely hide the given text when tested on Tesseract 4.0</p>
</figcaption>
</figure>

<p>Initially, the algorithm worked using only <strong>overtext</strong> mode, which applied noise in the rectangle which contained characters. However, this method is not the best choice for texts written using smaller characters mainly because there are less pixels that can be altered thus drastically lowering the readability even with minimal amounts of noise. For this special case, the decision to insert the noise in-between the text rows (<strong>artifacts</strong>) was taken in order to preserve the original characters. Both methods presented similar success rates in hiding texts from the targeted OCR system.</p>

<p>Just for fun, here‚Äôs what happens if the score function is inverted, which translates as ‚Äúgenerate an image with as much noise as possible, but which can be read by OCR software‚Äù. Weird, but it‚Äôs still recognized‚Ä¶</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/inverted-function.png" alt="Tesseract recognized the original text with **no errors**. How about you?" />
  <figcaption><p>Tesseract recognized the original text with <strong>no errors</strong>. How about you?</p>
</figcaption>
</figure>

<h2 id="results-on-tesseract">Results on Tesseract</h2>

<p>Promising results were achieved while testing against Tesseract 4.0. In the following figure is presented an early (non-final) sample in which the word ‚Äú<strong>Random</strong>‚Äù is not recognized by Tesseract:</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/tess-results-ui.png" alt="The first word is successfully hidden from the OCR system" />
  <figcaption><p>The first word is successfully hidden from the OCR system</p>
</figcaption>
</figure>

<h2 id="tests-on-googles-cloud-vision-platform">Tests on Google‚Äôs Cloud Vision Platform</h2>

<p>This is where things get interesting.</p>

<h5 id="the-implemented-score-function-can-be-maximized-in-2-ways-hiding-characters-or-tricking-the-ocr-engine-into-adding-characters-which-shouldnt-be-there">The implemented score function can be maximized in 2 ways: hiding characters or tricking the OCR engine into adding characters which shouldn‚Äôt be there.</h5>

<p>One of the samples managed to create a <strong>loop</strong> in the recognition process of <strong>Google‚Äôs Cloud Vision OCR</strong>, basically recognizing the same text multiple times. No <strong>DoS</strong> or anything (or I‚Äôm not aware of it), I‚Äôm still not sure if the loop persisted or not - it either produced a small number of iterations, failed (timed out?) or they had load balancers which compensated for this and used different instances.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/cloud_ocr_bug.png" alt="Possible loop in the recognition process: the same text gets recognized multiple times. The bottom-left and the top-right corners are 'merged' into an oblique text row so the recognition process is sent back to already processed text." />
  <figcaption><p>Possible loop in the recognition process: the same text gets recognized multiple times. The bottom-left and the top-right corners are ‚Äòmerged‚Äô into an oblique text row so the recognition process is sent back to already processed text.</p>
</figcaption>
</figure>

<p>Let‚Äôs take a closer look at the sample: below, you can see how the adversarial sample was interpreted by Google‚Äôs Cloud Vision OCR system. The image was submitted directly to the Cloud Vision platform via the <a href="https://cloud.google.com/vision/" rel="nofollow">‚ÄúTry the API‚Äù</a> option so, at the moment of testing, the results could be easily reproduced.</p>
<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/cloud_ocr_bug2.png" alt="Rectangles returned by Cloud Vision indicate that additional text rows are 'created' during the recognition thus creating a loop" />
  <figcaption><p>Rectangles returned by Cloud Vision indicate that additional text rows are ‚Äòcreated‚Äô during the recognition thus creating a loop</p>
</figcaption>
</figure>

<p>Also the ‚Äòboring‚Äô case where the characters are hidden:</p>
<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/cloud-ocr-artifacts.png" alt="Once again, using the artifacts mode on a small text since larger texts are way easier to hide" />
  <figcaption><p>Once again, using the artifacts mode on a small text since larger texts are way easier to hide</p>
</figcaption>
</figure>

<h2 id="conclusions">Conclusions</h2>

<p>It works, but the project reached its objective and is no longer in development.
It seems difficult to create samples that work for all OCR systems (<strong>generalization</strong>).</p>

<p>Also, the samples are vulnerable to changes at the <strong>preprocessing</strong> stage in the OCR pipeline such as:</p>

<ul>
  <li>noise filtering (e.g.: median filters)</li>
  <li>compression techniques (e.g.: Fourier compression)</li>
  <li>downscaling-&gt;upscaling (e.g.: Autoencoders)</li>
</ul>

<p>However, we can conclude that, using this approach, it is more challenging to mask small characters without making the text difficult to read. I compiled the following graph, in which are compared: the images generated by the algorithm (below <strong>7%</strong> noise density) and a set of images that contain random noise (<strong>15%</strong> noise density). The 2 sets contain different images with characters of sizes: 12, 21, 36, 50. Each random noise set contains 62 samples for each size - average values were used.</p>

<p><strong>Noise efficiency</strong> is computed by taking into account the <strong>Levenshtein distance</strong> and the total <strong>amount of noise</strong> in the image.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/noise-eff-cloudocr.png" alt="As characters get smaller, the efficiency of the noise added by the algorithm decreases - the random noise samples behave in an opposite manner." />
  <figcaption><p>As characters get smaller, the efficiency of the noise added by the algorithm decreases - the random noise samples behave in an opposite manner.</p>
</figcaption>
</figure>

<h2 id="interesting-todos">Interesting TODO‚Äôs</h2>

<ul>
  <li>Extracting templates from samples and training a generator?</li>
  <li>Exploiting directly the row segmentation feature?</li>
  <li>Attacking Otsu‚Äôs binarization method?</li>
</ul>

<p>Maybe someday‚Ä¶</p>

:ET