I"K_<p>This article discusses handwritten character recognition (<strong>OCR</strong>) in images using <em>sequence-to-sequence</em> (<strong>seq2seq</strong>) mapping performed by a <em>Convolutional Recurrent Neural Network</em> (<strong>CRNN</strong>) trained with <em>Connectionist Temporal Classification</em> (<strong>CTC</strong>) loss. The aforementioned approach is employed in multiple modern OCR engines for handwritten text (e.g., <a href="https://arxiv.org/pdf/1902.10525.pdf" rel="nofollow">Google’s Keyboard App</a> - convolutions are replaced with Bezier interpolations) or typed text (e.g., <a href="https://github.com/tesseract-ocr/docs/blob/master/das_tutorial2016/6ModernizationEfforts.pdf" rel="nofollow">Tesseract 4’s CRNN Based Recognition Module</a>).</p>

<p>For the sake of simplicity, the example I’ll be presenting performs only digit recognition but can be easily extended to accommodate more classes of characters.</p>

<h5 id="im-providing-a-google-colab-document-that-includes-a-working-example-of-the-concept-presented-here">I’m providing a <a href="https://colab.research.google.com/drive/1VRyObLgslpzeB33xITPdm_3E2cAxLuX3?usp=sharing" rel="nofollow">Google Colab</a> document that includes a working example of the concept presented here.</h5>

<h2 id="previous-inadequacies-and-justification">Previous Inadequacies and Justification</h2>

<blockquote>
  <p>“Why not simply segment characters in the image and recognize them one by one?”</p>
</blockquote>

<p>While the approach is, indeed, more straightforward and has been incorporated in older OCR engines, it has its caveats, especially when considering handwritten text. These are caused by the imperfections of the written characters which can produce segmentation issues thus attempting to recognize invalid glyphs or symbols. Consider the following images for clarification:</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/pytorch-crnn-seq2seq-digits-recognition/fragmented-characters.png" alt="A fragmented '5' is segmented as 2 different characters that are later passed to the recognition module. " />
  <figcaption><p>A fragmented ‘5’ is segmented as 2 different characters that are later passed to the recognition module.</p>
</figcaption>
</figure>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/pytorch-crnn-seq2seq-digits-recognition/merged-characters.png" alt="The first 2 digits are 'merged' together and considered a single character by both segmentation mechanism and OCR engine." />
  <figcaption><p>The first 2 digits are ‘merged’ together and considered a single character by both segmentation mechanism and OCR engine.</p>
</figcaption>
</figure>

<p>Whereas the MNIST problem is considered solved thus implying that reliable classifiers can be constructed to individually recognize digits, the problem of correct segmentation still remains in realistic scenarios. Splitting or merging glyphs to form valid digits proves to be a difficult challenge and requires additional knowledge to be embedded into the segmentation module.</p>

<h2 id="seq2seq-classifications">Seq2Seq Classifications</h2>

<p>In this context, the main advantage brought by a <strong>seq2seq</strong> classifier is that it diminishes the impact of erroneous segmentations and takes advantage of the ability of a neural network to generalize. It only requires a valid segmentation of the word or text line in cause.</p>

<p>Consider the following simplistic model that has a <strong>sliding window</strong> or <strong>mask</strong> (no convolutions), of size <code class="highlighter-rouge">(1, img_height)</code>. Each set of pixels covered by the sliding window is fed into a neural network made out of neurons with <strong>memory</strong> (e.g., <strong>GRU</strong> or <strong>LSTM</strong>); the job of the neural network is to take a sequence of such stripes and output recognized digits. Take a look at the following figure:</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/pytorch-crnn-seq2seq-digits-recognition/one-digit-rnn.png" alt="The RNN learns to recognize the digit '5' only by seeing stripes of width equal to 1 of the digit in cause - think of it as time series; by combining information from previous and current inputs, the RNN can determine the correct class." />
  <figcaption><p>The RNN learns to recognize the digit ‘5’ only by seeing stripes of width equal to 1 of the digit in cause - think of it as time series; by combining information from previous and current inputs, the RNN can determine the correct class.</p>
</figcaption>
</figure>

<p>Multiple digits will be included in a single sequence - because we’re feeding the network an image which contains more than a digit. It is up to the neural network to determine during the training phase how many stripes to take into account when classifying a digit (i.e., how much to memorize). The image below illustrates how a RNN should ‘group’ stripes together in order to recognize each digit in the sequence.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/pytorch-crnn-seq2seq-digits-recognition/rnn-ctc-ocr.png" alt="The RNN receives sequences of 'vertical' arrays of pixels (stripes) covered by the sliding window of width equal to 1; once trained, the RNN will be able to memorize that certain sequences of arrays (here in colors) form specific digits and properly separate multiple digits (i.e., 'change the colors') even though they are merged in the given image." />
  <figcaption><p>The RNN receives sequences of ‘vertical’ arrays of pixels (stripes) covered by the sliding window of width equal to 1; once trained, the RNN will be able to memorize that certain sequences of arrays (here in colors) form specific digits and properly separate multiple digits (i.e., ‘change the colors’) even though they are merged in the given image.</p>
</figcaption>
</figure>

<p>Using this method, it is possible to train a neural network by simply saying that the image above contains the numbers ‘55207’, without further information (e.g.: alignment, delimitations, bounding boxes etc.)</p>

<h2 id="ctc-and-duplicates-removal">CTC and Duplicates Removal</h2>

<p>CTC loss is most commonly employed to train seq2seq RNNs.</p>

<p>It works by <strong>summing</strong> the <strong>probabilities for all possible alignments</strong>; the <strong>probability of an alignment</strong> is determined by <strong>multiplying</strong> the probabilities of having specific digits in certain slots. An alignment can be seen as a plausible sequence of recognized digits.</p>

<p>Going back to the <strong>‘55207’</strong> example above, we can express the probability of the alignment \(A_55207\) as follows:</p>

<p>[P(A_55207) = P(A_1 = 5) \cdot P(A_2 = 5) \ cdot P(A_3 = 2) \cdot P(A_4 = 0) \ cdot P(A_5 = 7)]</p>

<p>To properly remove duplicates and correctly handle numbers that contain repeating digits, the <strong>blank</strong> class is introduced, with the following rules:</p>
<ul>
  <li>2 (or more) repeating digits are collapsed into a single instance of that digit unless separated by <strong>blank</strong></li>
</ul>

<p>It is correct to observe that the RNN must output a class for each stripe given as input therefore the following issues might appear:</p>
<ul>
  <li>handling duplicate characters ()</li>
</ul>

<p>In this work I took a look at Tesseract 4’s performance at recognizing characters from a challenging dataset and proposed a minimalistic convolution-based approach for input image preprocessing that can boost the character-level <strong>accuracy</strong> from <strong>13.4%</strong> to <strong>61.6%</strong> (+359% relative change), and the <strong>F1 score</strong> from <strong>16.3%</strong> to <strong>72.9%</strong> (+347% relative change) on the aforementioned dataset. The convolution kernels are determined using reinforcement learning; moreover, to simulate the lack of ground truth in realistic scenarios, the <strong>training set</strong> consists of only <strong>30</strong> images while the <strong>testing set</strong> includes <strong>10,000</strong>.</p>

<p>The dataset in cause is called <a href="https://pero.fit.vutbr.cz/brno_mobile_ocr_dataset" rel="nofollow">Brno Mobile</a>, and contains colored photographs of typed text, taken with handheld devices. Factors such as blurriness, low resolution, contrast, brightness are contributing to making the images challenging for an OCR engine.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/dataset-sample.webp" alt="Resized image from the Brno dataset which contains text that was not recognized by Tesseract 4 during the evaluation (an empty string was returned)" />
  <figcaption><p>Resized image from the Brno dataset which contains text that was not recognized by Tesseract 4 during the evaluation (an empty string was returned)</p>
</figcaption>
</figure>

<p>During this experiment, the <em>out of the box</em> version of Tesseract 4 has been used, which implies:</p>
<ul>
  <li>no retraining of the OCR engine</li>
  <li>no lexicon / dictionary augmentations</li>
  <li>no hints about the language used in the dataset</li>
  <li>no hints about segmentation methods; default (automatic) segmentation is used</li>
  <li>default settings for the recognition engine (LSTM + Tesseract)</li>
</ul>

<h2 id="problem-analysis">Problem Analysis</h2>

<p>Tesseract 4 has proven great performance when tested on favorable datasets by achieving good balance between precision and recall. It is presumed that this evaluation is performed on images that resemble scanned documents or book pages (with or without additional preprocessing) in which the number of camera-caused distortions is minimal. Tests on the Brno dataset led to much worse performance that will be discussed later in the article.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/tesseract-stats.webp" alt="Tesseract 4's performance when evaluated using the Google Books Dataset - taken from [DAS 2016](https://github.com/tesseract-ocr/docs/tree/master/das_tutorial2016){:rel='nofollow'}" />
  <figcaption><p>Tesseract 4’s performance when evaluated using the Google Books Dataset - taken from <a href="https://github.com/tesseract-ocr/docs/tree/master/das_tutorial2016" rel="nofollow">DAS 2016</a></p>
</figcaption>
</figure>

<p>In the above figure, a high <strong>precision</strong> indicates favorable <em>True-Positives</em> to <em>False-Positives</em> ratio thus revealing proper differentiation between characters (i.e. a relatively small number of misclassifications). Despite this, almost no improvements in <strong>recall</strong> can be observed when switching from the <strong>base</strong> classification method to the <em>Long Short-Term Memory</em> (<strong>LSTM</strong>) based <em>Convolutional Recurrent Neural Network</em> (<strong>CRNN</strong>) for <em>sequence to sequence</em> mapping.</p>

<blockquote>
  <p>“Despite being designed over 20 years ago, the current Tesseract classifier is incredibly difficult to beat with so-called modern methods.” - Ray Smith, author of Tesseract</p>
</blockquote>

<p>I assume that further training for different fonts might not provide significant improvements and neither will a different model of classifier. <em>Is there a chance that the classifier doesn’t receive the correct input?</em></p>

<p>It was pointed out in a previous article that <a href="https://codingvision.net/ai/evaluating-the-robustness-of-ocr-systems">Tesseract is not robust to noise</a>; certain <em>salt-and-pepper</em> noise patterns disrupt the character recognition process, leading to large segments of text being completely ignored by the OCR engine - the infamous <strong>empty string</strong>. From empirical observations, these errors seem to occur either for a whole word or sentence or not at all thus suggesting a weakness in the segmentation methodology.</p>

<p>The existence of similar behavior, given images which present more natural distortions, is questioned - hence this experiment.</p>

<h2 id="black-box-considerations">Black-box Considerations</h2>

<p>Since analyzing Tesseract’s segmentation methods is a daunting task, I opted for an adaptive external image correction method. To avoid diving into Tesseract 4’s source code, the OCR engine is considered a black-box; in this case, an unsupervised learning method must be employed. This ensures easier transitions to other OCR engines as it doesn’t directly rely on concrete implementations but only on outputs - at the cost of processing power and optimality.</p>

<h2 id="proposed-solution">Proposed Solution</h2>
<p>The solution consists in directly preprocessing images before they are fed to Tesseract 4. An adaptive preprocessing operation is required, in order to properly compensate for any image features that cause problems in the segmentation process. In other words, an input image must be adapted so it complies with Tesseract 4’s preferences and maximizes the chance of producing the correct output, preferably without performing down-sampling.</p>

<p>I choose a convolution-based approach for flexibility and speed; other articles tend to perform more rigid image adjustments (such as global changes in brightness, fixed-constant conversion to grayscale, histogram equalization, etc.). I preferred an approach that can properly learn to highlight or mask regions of the image according to various features. For this, the kernels are optimized using reinforcement learning using an actor-critic model. To be more specific, it relies on <em>Twin Delayed Deep Deterministic Policy Gradient</em> (<strong>TD3</strong> for short), for discovering features which minimize the <em>Levenshtein distance</em> between the <strong>recognized text</strong> and the <strong>ground truth</strong>. I’ll not dive into implementation details of TD3 here as it would be somehow out of scope but think of it as a method of optimizing the following formula:</p>

<p>[\max_{K1,K2,K3,K4,K5}\sum_{i=1}^{N}{-Levenshtein(OCR(Image_i * K1 * K2 * K3 * K4 * K5),Text_i)}]</p>

<p>Where \(K_j\) is a kernel, and \(&lt;Image_i, Text_i&gt;\) is a tuple from the training set.</p>

<h5 id="a-short-simpler-proof-of-concept-of-the-convolutional-preprocessor-is-presented-in-this-google-colab-it-uses-a-different-architecture-than-the-final-one-and-has-the-purpose-of-verifying-if-the-idea-of-using-convolutions-is-feasible-and-offers-good-results-a-comparison-is-presented-between-original-and-preprocessed-images-including-recognized-texts-for-each-sample">A short (simpler) proof of concept of the convolutional preprocessor is presented in <a href="https://colab.research.google.com/drive/1l0qT2S3tkY4WHTRbkVK_J5jATPg0t41-?usp=sharing" rel="nofollow">this Google Colab</a>. It uses a different architecture than the final one and has the purpose of verifying if the idea of using convolutions is feasible and offers good results. A comparison is presented between original and preprocessed images including recognized texts for each sample.</h5>

<p>The final model is illustrated below, with <strong>ReLU</strong> activations after each convolution to capture nonlinearities and prevent having negative values as pixels’ colors.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/convolutional-preprocessor.webp" alt="Architecture of the Convolutional Preprocessor used to adapt images for Tesseract 4" />
  <figcaption><p>Architecture of the Convolutional Preprocessor used to adapt images for Tesseract 4</p>
</figcaption>
</figure>

<p>To properly compensate for image coloring and reduce the number of channels (<span style="color:red">R</span>, <span style="color:green">G</span>, <span style="color:blue">B</span>), 1x1 convolutions are used. This prevents overfitting up to a point while also ensuring grayscale output. Further convolutions are applied only on the grayscale image.</p>

<p><em>Symmetry constraints</em> are additionally enforced for each 3x3 kernel in order to minimize the number of trainable parameters and avoid overfitting. This means that for a 3x3 kernel only 6 variables out of 9 must be determined while the rest can be generated through <em>mirroring</em>. Below are the values I got for the five kernels (bold to emphasize symmetry):</p>

<table class="data-table">
  <thead>
    <tr>
      <th>#1</th>
      <th>#2</th>
      <th> </th>
      <th> </th>
      <th>#3</th>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><span style="color:red"><strong>0.7</strong></span></td>
      <td>0.2573</td>
      <td>-0.3</td>
      <td>0.3</td>
      <td>0.3</td>
      <td><strong>-0.2996</strong></td>
      <td>0.3</td>
    </tr>
    <tr>
      <td><span style="color:green"><strong>1.3</strong></span></td>
      <td><strong>0.3</strong></td>
      <td><strong>1.3</strong></td>
      <td><strong>-0.295</strong></td>
      <td>0.3</td>
      <td><strong>1.2949</strong></td>
      <td>0.3</td>
    </tr>
    <tr>
      <td><span style="color:blue"><strong>1.3</strong></span></td>
      <td>0.2573</td>
      <td>-0.3</td>
      <td>0.3</td>
      <td>-0.2802</td>
      <td><strong>0.2922</strong></td>
      <td>-0.2802</td>
    </tr>
  </tbody>
</table>

<table class="data-table">
  <thead>
    <tr>
      <th>#4</th>
      <th> </th>
      <th> </th>
      <th>#5</th>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>-0.2793</strong></td>
      <td>0.2395</td>
      <td>0.2885</td>
      <td>-0.294</td>
      <td>-0.2905</td>
      <td><strong>-0.2939</strong></td>
    </tr>
    <tr>
      <td>0.2395</td>
      <td><strong>0.7119</strong></td>
      <td>0.3</td>
      <td>0.3</td>
      <td><strong>1.162</strong></td>
      <td>-0.2905</td>
    </tr>
    <tr>
      <td>0.2885</td>
      <td>0.3</td>
      <td><strong>-0.2828</strong></td>
      <td><strong>-0.2328</strong></td>
      <td>0.3</td>
      <td>-0.294</td>
    </tr>
  </tbody>
</table>

<h2 id="preprocessing-results">Preprocessing Results</h2>

<p>I extracted the image from each convolution layer and clamped its values to the <em>0-255</em> interval to properly view each transformation:</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/transformations.webp" alt="Transformations of an image as it passes through the convolutional preprocessor, viewed from left (original) to right (final sample); observe the removal of incomplete characters from the upper-left region" />
  <figcaption><p>Transformations of an image as it passes through the convolutional preprocessor, viewed from left (original) to right (final sample); observe the removal of incomplete characters from the upper-left region</p>
</figcaption>
</figure>

<h2 id="comparison">Comparison</h2>

<p>I used 10,000 images from the testing set for the evaluation of the current methodology and compiled the following graphs. The differences between original and preprocessed samples are illustrated with three metrics of interest: <em>Character Error Rate</em> (<strong>CER</strong>), <em>Word Error Rate</em> (<strong>WER</strong>) and <em>Longest Common Subsequence Error</em> (<strong>LCSE</strong>). In this article, <strong>LCSE</strong> is computed as follows:</p>

<table>
  <tbody>
    <tr>
      <td>[LCSE(Text_1,Text_2 )=</td>
      <td>Text_1</td>
      <td>-</td>
      <td>LCS(Text_1,Text_2 )</td>
      <td>+</td>
      <td>Text_2</td>
      <td>-</td>
      <td>LCS(Text_1,Text_2 )</td>
      <td>]</td>
    </tr>
  </tbody>
</table>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/results-comparison.webp" alt="&lt;span style='color:green'&gt;Preprocessed&lt;/span&gt; vs &lt;span style='color:red'&gt;Original&lt;/span&gt; Images from the testing set; lower is better for each metric; dashed lines represent first degree approximations using least squares regression for the ease of interpretation" />
  <figcaption><p><span style="color:green">Preprocessed</span> vs <span style="color:red">Original</span> Images from the testing set; lower is better for each metric; dashed lines represent first degree approximations using least squares regression for the ease of interpretation</p>
</figcaption>
</figure>

<p>Additionally, I plotted everything in histogram format to properly see the distributions of errors. For <strong>CER</strong> and <strong>WER</strong>, it is easy to observe the spikes around <strong>1</strong> (100%) that suggest the aforementioned segmentation problem (at block-of-text level) produces the most frequent error (<strong>empty strings</strong> are returned so all characters are wrong). In certain situations, the <strong>WER</strong> is larger than <strong>1</strong> because the preprocessing step introduces artifacts near the border of the image thus leading to recognition of non-existent characters. When looking at the <strong>LCSE</strong> plot, a distribution shift can be seen from the original approximately gaussian shape with its peak (mode) near the average number of characters in an image (<strong>56.95</strong>) to a more favorable shape with overall lower error rates.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/results-distributions.webp" alt="&lt;span style='color:green'&gt;Preprocessed&lt;/span&gt; vs &lt;span style='color:red'&gt;Original&lt;/span&gt; Images from the testing set; comparison of distributions of errors" />
  <figcaption><p><span style="color:green">Preprocessed</span> vs <span style="color:red">Original</span> Images from the testing set; comparison of distributions of errors</p>
</figcaption>
</figure>

<p>A numeric comparison is presented below:</p>

<table class="data-table">
  <thead>
    <tr>
      <th>Metric</th>
      <th>Original (Avg.)</th>
      <th>Preprocessed (Avg.)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CER</td>
      <td>0.866</td>
      <td>0.384</td>
    </tr>
    <tr>
      <td>WER</td>
      <td>0.903</td>
      <td>0.593</td>
    </tr>
    <tr>
      <td>LCSE</td>
      <td>48.834</td>
      <td>24.987</td>
    </tr>
    <tr>
      <td>Precision</td>
      <td>0.155</td>
      <td>0.725</td>
    </tr>
    <tr>
      <td>Recall</td>
      <td>0.172</td>
      <td>0.734</td>
    </tr>
    <tr>
      <td>F1 Score</td>
      <td>0.163</td>
      <td>0.729</td>
    </tr>
  </tbody>
</table>

<h2 id="takeaways">Takeaways</h2>

<p>Significant improvements can be observed through this preprocessing operation. Moreover, the majority of errors probably do not occur in the <em>sequence to sequence</em> classifier (since all the recognized characters are erroneous and would contradict previous performance analysis). A page-segmentation issue when automatic mode is used seems more plausible. It is shown that an array of convolutions is sufficient, in this case, to decrease error rates substantially.</p>

<p>The OCR performance on the preprocessed images is overall better but not good enough to be reliable. A 38% character error rate is still a large setback. I’m pretty sure that better recognitions can be obtained with more fine-tuning, a more complex architecture for the convolutional preprocessor and a more diverse training set. However, the current implementation is already very slow to train which makes me question if the entire methodology is feasible from this point of view.</p>

:ET