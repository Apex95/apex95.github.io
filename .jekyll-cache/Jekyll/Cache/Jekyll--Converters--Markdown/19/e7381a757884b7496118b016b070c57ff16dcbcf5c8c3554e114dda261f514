I"úE<p>In this article, I‚Äôm going to discuss about my Bachelor‚Äôs degree final project, which is about evaluating the robustness of <strong>OCR systems</strong> (such as <strong>Tesseract</strong> or <strong>Google‚Äôs Cloud Vision</strong>) when adversarial samples are presented as inputs. It‚Äôs somewhere in-between <strong>fuzzing</strong> and <strong>adversarial samples crafting</strong>, on a black box, the main objective being the creation of <strong>OCR-proof</strong> images, with minimal amounts of noise.</p>

<p>It‚Äôs an old project that I recently presented at an <a href="https://spritz.math.unipd.it/events/2019/PIU2019/PagesOutput/SSS/index.html" rel="nofollow">International Security Summer School</a> hosted by the University of Padua. I decided to also publish it here mainly because of the positive feedback received when presented at the summer school.</p>

<p>I‚Äôll try to focus on methodology and results, which I consider being of interest, without diving into implementation details.</p>

<h5 id="i-published-this-1-year-ago---not-sure-if-it-still-works-as-described-here-hopefully-it-does-but-im-pretty-sure-google-made-changes-to-the-vision-engine-since-then">I published this ~1 year ago - not sure if it still works as described here. Hopefully it does, but I‚Äôm pretty sure Google made changes to the Vision engine since then.</h5>

<h2 id="motivation">Motivation</h2>

<p>Let‚Äôs start with what I considered to be plausible use cases for this project and what problems it would be able to solve.</p>

<ul>
  <li>
    <p><strong>Confidentiality</strong> of text included in images? ‚Äì It is no surprise to us that large services (that‚Äôs you, Google) will scan hosted images for texts in order to improve classification or extract user information. We might want some of that information to remain private.</p>
  </li>
  <li>
    <p>Smart <strong>CAPTCHA</strong>? ‚Äì This aims to improve the efficiency of CAPTCHAs by creating images which are easier to read by humans, thus reducing the discomfort, while also rendering OCR-based bots ineffective.</p>
  </li>
  <li>
    <p>Defense against <strong>content generators</strong>? ‚Äì This could serve as a defense mechanism against programs which scan documents and republish content (sometimes using different names) in order to gain undeserved merits.</p>
  </li>
</ul>

<h2 id="challenges">Challenges</h2>

<p>Now, let‚Äôs focus on the different constraints and challenges:</p>

<h3 id="1-complex--closed-source-architecture">1. Complex / closed-source architecture</h3>

<figure class="image">
  &lt;img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/tess-pipeline.png" alt="<p>Tesseract‚Äôs pipeline as <a href="https://github.com/tesseract-ocr/docs/blob/master/das_tutorial2016/2ArchitectureAndDataStructures.pdf" rel="nofollow">presented at DAS 2016</a></p>
"&gt;
  <figcaption><p>Tesseract‚Äôs pipeline as <a href="https://github.com/tesseract-ocr/docs/blob/master/das_tutorial2016/2ArchitectureAndDataStructures.pdf" rel="nofollow">presented at DAS 2016</a></p>
</figcaption>
</figure>

<p>Modern OCR systems are more complex than basic convolutional neural networks as they need to perform multiple actions (e.g.: deskewing, layout detection, text rows segmentation), therefore finding ways to correctly compute gradients is a daunting task. Moreover, many of them do not provide access to source code thus making it difficult to use techniques such as <strong>FGSM</strong> or <strong>GAN</strong>s.</p>

<h3 id="2-binarization">2. Binarization</h3>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/binarization.png" alt="&lt;p&gt;Result of the binarization procedure, using an adaptive threshold&lt;/p&gt;
" />
  <figcaption><p>Result of the binarization procedure, using an adaptive threshold</p>
</figcaption>
</figure>

<p>An OCR system usually applies a binarization procedure (e.g.: <strong>Otsu</strong>‚Äôs method) to the image before running it through the main classifier in order to separate the text from the background, the ideal output being pure black text on a clean white background.</p>

<p>This proves troublesome because it restricts the samples generator from altering pixels using small values: as an example, converting a black pixel to a grayish color will be reverted in the binarization process thus generating no feedback.</p>

<h3 id="3-adaptive-classification">3. Adaptive classification</h3>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/adaptive-classifier.png" alt="&lt;p&gt;Tesseract‚Äôs adaptive classifier incorrectly recognizes an ‚Äòh‚Äô as a ‚Äòb‚Äô, in the first image. In the second sample, Tesseract observes a correct ‚Äòh‚Äô character (confidence is larger than a threshold) adjusts the classifier‚Äôs configuration and correctly classifies the first ‚Äòh‚Äô&lt;/p&gt;
" />
  <figcaption><p>Tesseract‚Äôs adaptive classifier incorrectly recognizes an ‚Äòh‚Äô as a ‚Äòb‚Äô, in the first image. In the second sample, Tesseract observes a correct ‚Äòh‚Äô character (confidence is larger than a threshold) adjusts the classifier‚Äôs configuration and correctly classifies the first ‚Äòh‚Äô</p>
</figcaption>
</figure>

<p>This is specific to Tesseract, which is rather deprecated nowadays - still very popular, though. Modern classifiers might be using this method, too. It consists of performing 2 iterations over the same input image. In the first pass, characters which can be recognized with a certain confidence are selected and used as temporary training data. In the second pass, the OCR attempts to classify characters which were not recognized in the first iteration, but using what it previously learned.</p>

<p>Considering this, having an adversarial generator which alters one character at a time might not work as expected since that character might appear later in the image.</p>

<h3 id="4-lower-entropy">4. Lower entropy</h3>

<p>This refers to the fact that the input data is rather ‚Äòlimited‚Äô for an OCR system when compared to‚Ä¶ let‚Äôs say object recognition. As an example, images which contain 3D objects have larger variance than those which contain characters since the characters have a rather fixed shape and format. This should make it more difficult to create adversarial samples for character classifiers without applying distortions.</p>

<p>A direct consequence is that it greatly restricts the amount of noise that can be added to an image so that the readability is preserved.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/noise-readability.png" alt="&lt;p&gt;Applying noise in an image usually decreases readability, which is not what we want here&lt;/p&gt;
" />
  <figcaption><p>Applying noise in an image usually decreases readability, which is not what we want here</p>
</figcaption>
</figure>

<h3 id="5-dictionaries">5. Dictionaries</h3>

<p>OCR systems will attempt to improve their accuracy by employing dictionaries with predefined words. Altering a single character in a word (i.e.: the incremental approach) might not be effective in this case.</p>

<h2 id="targeted-ocr-systems">Targeted OCR Systems</h2>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/tesseract-gocr.png" alt="&lt;p&gt;Tested locally on Tesseract 4.0 and remotely on Google‚Äôs Cloud Vision OCR&lt;/p&gt;
" />
  <figcaption><p>Tested locally on Tesseract 4.0 and remotely on Google‚Äôs Cloud Vision OCR</p>
</figcaption>
</figure>

<p>For this project, I used <strong>Tesseract 4.0</strong> for prototyping and testing, as it had no timing restrictions and allowed me to run a fast, parallel model with high throughput so I could test if the implementation works as expected. Later, I moved to <strong>Google‚Äôs Cloud Vision OCR</strong> and tried some ‚Äòremote‚Äô fuzzing through the API.</p>

<h2 id="methodology">Methodology</h2>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/architecture.png" alt="&lt;p&gt;A rather simplified view of the flow; a feedback-based adversarial samples generator (in image: obfuscator) alters inputs in order to maximize the error of the OCR system&lt;/p&gt;
" />
  <figcaption><p>A rather simplified view of the flow; a feedback-based adversarial samples generator (in image: obfuscator) alters inputs in order to maximize the error of the OCR system</p>
</figcaption>
</figure>

<p>In order to be able to cover even black box cases, I used a <strong>genetic algorithm</strong> guided by the feedback of the targeted OCR system. We observe that the confidence of the classifier, alone, is not a good metric for this problem, a score function based on the <a href="https://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">Levenshtein distance</a> and the <strong>amount of noise</strong> is employed.</p>

<p>One of the main problems here was the size of the search space which was partially solved by identifying regions of interest in the image and focusing only on these. Also, lots of parameter tuning‚Ä¶</p>

<h2 id="noise-properties">Noise properties</h2>

<p>Given the constraints, the following properties of the noise model must be matched:</p>

<ul>
  <li><strong>high contrast</strong> ‚Äì so it bypasses the binarization process and generates feedback</li>
  <li><strong>low density</strong> ‚Äì in order to maintain readability by exploiting the natural <strong>low-filtering</strong> capability of the human vision</li>
</ul>

<p>Applying <strong>salt-and-pepper</strong> noise in a smart manner will, hopefully, satisfy the constraints.</p>

<h2 id="working-modes">Working modes</h2>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/modes.png" alt="&lt;p&gt;Different working modes for small and large characters, in order to preserve readability. Both managed to entirely hide the given text when tested on Tesseract 4.0&lt;/p&gt;
" />
  <figcaption><p>Different working modes for small and large characters, in order to preserve readability. Both managed to entirely hide the given text when tested on Tesseract 4.0</p>
</figcaption>
</figure>

<p>Initially, the algorithm worked using only <strong>overtext</strong> mode, which applied noise in the rectangle which contained characters. However, this method is not the best choice for texts written using smaller characters mainly because there are less pixels that can be altered thus drastically lowering the readability even with minimal amounts of noise. For this special case, the decision to insert the noise in-between the text rows (<strong>artifacts</strong>) was taken in order to preserve the original characters. Both methods presented similar success rates in hiding texts from the targeted OCR system.</p>

<p>Just for fun, here‚Äôs what happens if the score function is inverted, which translates as ‚Äúgenerate an image with as much noise as possible, but which can be read by OCR software‚Äù. Weird, but it‚Äôs still recognized‚Ä¶</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/inverted-function.png" alt="&lt;p&gt;Tesseract recognized the original text with &lt;strong&gt;no errors&lt;/strong&gt;. How about you?&lt;/p&gt;
" />
  <figcaption><p>Tesseract recognized the original text with <strong>no errors</strong>. How about you?</p>
</figcaption>
</figure>

<h2 id="results-on-tesseract">Results on Tesseract</h2>

<p>Promising results were achieved while testing against Tesseract 4.0. In the following figure is presented an early (non-final) sample in which the word ‚Äú<strong>Random</strong>‚Äù is not recognized by Tesseract:</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/tess-results-ui.png" alt="&lt;p&gt;The first word is successfully hidden from the OCR system&lt;/p&gt;
" />
  <figcaption><p>The first word is successfully hidden from the OCR system</p>
</figcaption>
</figure>

<h2 id="tests-on-googles-cloud-vision-platform">Tests on Google‚Äôs Cloud Vision Platform</h2>

<p>This is where things get interesting.</p>

<h5 id="the-implemented-score-function-can-be-maximized-in-2-ways-hiding-characters-or-tricking-the-ocr-engine-into-adding-characters-which-shouldnt-be-there">The implemented score function can be maximized in 2 ways: hiding characters or tricking the OCR engine into adding characters which shouldn‚Äôt be there.</h5>

<p>One of the samples managed to create a <strong>loop</strong> in the recognition process of <strong>Google‚Äôs Cloud Vision OCR</strong>, basically recognizing the same text multiple times. No <strong>DoS</strong> or anything (or I‚Äôm not aware of it), I‚Äôm still not sure if the loop persisted or not - it either produced a small number of iterations, failed (timed out?) or they had load balancers which compensated for this and used different instances.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/cloud_ocr_bug.png" alt="&lt;p&gt;Possible loop in the recognition process: the same text gets recognized multiple times. The bottom-left and the top-right corners are ‚Äòmerged‚Äô into an oblique text row so the recognition process is sent back to already processed text.&lt;/p&gt;
" />
  <figcaption><p>Possible loop in the recognition process: the same text gets recognized multiple times. The bottom-left and the top-right corners are ‚Äòmerged‚Äô into an oblique text row so the recognition process is sent back to already processed text.</p>
</figcaption>
</figure>

<p>Let‚Äôs take a closer look at the sample: below, you can see how the adversarial sample was interpreted by Google‚Äôs Cloud Vision OCR system. The image was submitted directly to the Cloud Vision platform via the <a href="https://cloud.google.com/vision/" rel="nofollow">‚ÄúTry the API‚Äù</a> option so, at the moment of testing, the results could be easily reproduced.</p>
<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/cloud_ocr_bug2.png" alt="&lt;p&gt;Rectangles returned by Cloud Vision indicate that additional text rows are ‚Äòcreated‚Äô during the recognition thus creating a loop&lt;/p&gt;
" />
  <figcaption><p>Rectangles returned by Cloud Vision indicate that additional text rows are ‚Äòcreated‚Äô during the recognition thus creating a loop</p>
</figcaption>
</figure>

<p>Also the ‚Äòboring‚Äô case where the characters are hidden:</p>
<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/cloud-ocr-artifacts.png" alt="&lt;p&gt;Once again, using the artifacts mode on a small text since larger texts are way easier to hide&lt;/p&gt;
" />
  <figcaption><p>Once again, using the artifacts mode on a small text since larger texts are way easier to hide</p>
</figcaption>
</figure>

<h2 id="conclusions">Conclusions</h2>

<p>It works, but the project reached its objective and is no longer in development.
It seems difficult to create samples that work for all OCR systems (<strong>generalization</strong>).</p>

<p>Also, the samples are vulnerable to changes at the <strong>preprocessing</strong> stage in the OCR pipeline such as:</p>

<ul>
  <li>noise filtering (e.g.: median filters)</li>
  <li>compression techniques (e.g.: Fourier compression)</li>
  <li>downscaling-&gt;upscaling (e.g.: Autoencoders)</li>
</ul>

<p>However, we can conclude that, using this approach, it is more challenging to mask small characters without making the text difficult to read. I compiled the following graph, in which are compared: the images generated by the algorithm (below <strong>7%</strong> noise density) and a set of images that contain random noise (<strong>15%</strong> noise density). The 2 sets contain different images with characters of sizes: 12, 21, 36, 50. Each random noise set contains 62 samples for each size - average values were used.</p>

<p><strong>Noise efficiency</strong> is computed by taking into account the <strong>Levenshtein distance</strong> and the total <strong>amount of noise</strong> in the image.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/noise-eff-cloudocr.png" alt="&lt;p&gt;As characters get smaller, the efficiency of the noise added by the algorithm decreases - the random noise samples behave in an opposite manner.&lt;/p&gt;
" />
  <figcaption><p>As characters get smaller, the efficiency of the noise added by the algorithm decreases - the random noise samples behave in an opposite manner.</p>
</figcaption>
</figure>

<h2 id="interesting-todos">Interesting TODO‚Äôs</h2>

<ul>
  <li>Extracting templates from samples and training a generator?</li>
  <li>Exploiting directly the row segmentation feature?</li>
  <li>Attacking Otsu‚Äôs binarization method?</li>
</ul>

<p>Maybe someday‚Ä¶</p>

:ET