I"(<p>In this work I took a look at Tesseract 4’s performance at recognizing characters from a challenging dataset and proposed a minimalistic convolution-based approach for input image preprocessing that can boost the character-level <strong>accuracy</strong> from <strong>13.4%</strong> to <strong>61.6%</strong> (+359% relative change), and the <strong>F1 score</strong> from <strong>16.3%</strong> to <strong>72.9%</strong> (+347% relative change) on the aforementioned dataset. The convolution kernels are determined using reinforcement learning; moreover, to simulate the lack of ground truth in realistic scenarios, the <strong>training set</strong> consists of only <strong>30</strong> images while the <strong>testing set</strong> includes <strong>10,000</strong>.</p>

<p>The dataset in cause is called <a href="https://pero.fit.vutbr.cz/brno_mobile_ocr_dataset" rel="nofollow">Brno Mobile</a>, and contains colored photographs of typed text, taken with handheld devices. Factors such as blurriness, low resolution, contrast, brightness are contributing to making the images challenging for an OCR engine.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/dataset-sample.png" alt="Resized image from the Brno dataset which contains text that was not recognized by Tesseract 4 during the evaluation (an empty string was returned)" />
  <figcaption><p>Resized image from the Brno dataset which contains text that was not recognized by Tesseract 4 during the evaluation (an empty string was returned)</p>
</figcaption>
</figure>

<p>During this experiment, the <em>out of the box</em> version of Tesseract 4 has been used, which implies:</p>
<ul>
  <li>no retraining of the OCR engine</li>
  <li>no lexicon / dictionary augmentations</li>
  <li>no hints about the language used in the dataset</li>
  <li>no hints about segmentation methods; default (automatic) segmentation is used</li>
</ul>

<h2 id="problem-analysis">Problem Analysis</h2>

<blockquote>
  <p>“Despite being designed over 20 years ago, the current Tesseract classifier is incredibly difficult to beat with so-called modern methods.” - Ray Smith, author of Tesseract</p>
</blockquote>

<p>Tesseract 4 has proven great performance when tested on favorable datasets by achieving good balance between precision and recall. It is presumed that this evaluation is performed on images that resemble scanned documents or book pages (with or without additional preprocessing) in which the number of camera-caused distortions is minimal. Tests on the Brno dataset led to much worse performance that will be discussed later in the article.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/tesseract-stats.png" alt="Tesseract 4's result when evaluated using the Google Books Dataset - taken from [DAS 2016](https://github.com/tesseract-ocr/docs/tree/master/das_tutorial2016){:rel='nofollow'}" />
  <figcaption><p>Tesseract 4’s result when evaluated using the Google Books Dataset - taken from <a href="https://github.com/tesseract-ocr/docs/tree/master/das_tutorial2016" rel="nofollow">DAS 2016</a></p>
</figcaption>
</figure>

<p>A high precision indicates favorable <em>True-Positives</em> to <em>False-Positives</em> ratio thus revealing proper differentiation between characters (i.e. a relatively small number of misclassifications). I assume that that further training for different fonts might not provide significant improvements and neither will a different model of classifier.</p>

<p>However, it was pointed out in a previous article that <a href="https://codingvision.net/ai/evaluating-the-robustness-of-ocr-systems">Tesseract is not robust to noise</a>; certain <em>salt-and-pepper</em> noise patterns disrupt the character recognition process, leading to large segments of text being completely ignored by the OCR engine - the infamous empty string. From empirical observations, these errors seem to occur either for a whole word or sentence or not at all thus suggesting a weakness in the segmentation methodology.</p>

<p>The existence of similar behavior, given images which present more natural distortions, is questioned - hence this experiment.</p>

<h2 id="blackbox-considerations">Blackbox Considerations</h2>

<p>Since analyzing Tesseract’s segmentation methods is a daunting task, I opted for an adaptive external image correction method. To avoid diving into Tesseract 4’s source code, the OCR engine is considered a black-box; in this case, an unsupervised learning method must be employed. This ensures compatibility with other OCR engines - since it doesn’t directly rely on concrete implementations but only on outputs - at the cost of processing power and optimality.</p>

<h2 id="proposed-solution">Proposed Solution</h2>
<p>The solution consists in directly preprocessing images before they are fed to Tesseract 4. An adaptive preprocessing operation is required, in order to properly compensate for any image features that cause problems in the segmentation process. In other words, an input image must be adapted so it complies with Tesseract 4’s preferences and maximizes the chance of producing the correct output.</p>

<p>I choose a convolution-based approach for flexibility and speed; the kernels are optimized using reinforcement learning using an actor-critic model. To be more specific, it relies on <em>Twin Delayed Deep Deterministic Policy Gradient</em> (<strong>TD3</strong> for short), for discovering features which minimize the <em>Character Error Rate</em> (<strong>CER</strong>). I’ll not dive into implementation details of TD3 here as it is somehow out of scope.</p>

<h5 id="a-short-simpler-proof-of-concept-is-presented-in-this-google-colab-it-uses-a-different-architecture-than-the-final-one-and-has-the-purpose-of-verifying-if-the-idea-of-using-convolutions-is-feasible-and-offers-good-results-a-comparison-is-presented-between-original-and-preprocessed-images-thus-revealing-tesseract-4s-segmentation-issue">A short (simpler) proof of concept is presented in <a href="https://colab.research.google.com/drive/1l0qT2S3tkY4WHTRbkVK_J5jATPg0t41-?usp=sharing" rel="nofollow">this Google Colab</a>. It uses a different architecture than the final one and has the purpose of verifying if the idea of using convolutions is feasible and offers good results. A comparison is presented between original and preprocessed images, thus revealing Tesseract 4’s segmentation issue.</h5>

<p>The final model is illustrated below, with <strong>ReLU</strong> activations in-between each convolution step to capture nonlinearities.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/convolutional-preprocessor.png" alt="Architecture of the Convolutional Preprocessor used to adapt images for Tesseract 4" />
  <figcaption><p>Architecture of the Convolutional Preprocessor used to adapt images for Tesseract 4</p>
</figcaption>
</figure>

<p>To properly compensate for image coloring and reduce the number of channels (<span style="color:red">R</span>, <span style="color:green">G</span>, <span style="color:blue">B</span>), 1x1 convolutions are used. The purpose is to reduce the number of variables while also ensuring grayscale output. Further convolutions are applied only on the grayscale image.</p>

<p><em>Symmetry constraints</em> are enforced for each 3x3 kernel in order to minimize the number of parameters and avoid <em>overfitting</em>. This means that for a 3x3 kernel only 6 variables out of 9 must be determined while the rest can be generated through mirroring.</p>

<table class="data-table">
  <thead>
    <tr>
      <th>#1</th>
      <th>#2</th>
      <th> </th>
      <th> </th>
      <th>#3</th>
      <th> </th>
      <th> </th>
      <th>#4</th>
      <th> </th>
      <th> </th>
      <th>#5</th>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.7</td>
      <td>0.2573</td>
      <td>-0.3</td>
      <td>0.3</td>
      <td>0.3</td>
      <td>-0.2996</td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>1.3</td>
      <td>0.3</td>
      <td>1.3</td>
      <td>-0.295</td>
      <td>0.3</td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>1.3</td>
      <td>0.2573</td>
      <td>-0.3</td>
      <td>0.3</td>
      <td>-0.2802</td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h2 id="comparison">Comparison</h2>

<p>I used 10,000 images from the testing set for the evaluation of the current methodology and compiled the following graphs. The difference between results is illustrated with three metrics of interest: <em>Character Error Rate</em> (<strong>CER</strong>), <em>Word Error Rate</em> (<strong>WER</strong>) and <em>Longest Common Subsequence Error</em> (<strong>LCSE</strong>).</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/results-comparison.png" alt="&lt;span style='color:green'&gt;Preprocessed&lt;/span&gt; vs &lt;span style='color:red'&gt;Original&lt;/span&gt; Images from the testing set; lower is better for each metric; dashed lines represent first degree approximations using least squares regression for the ease of interpretation" />
  <figcaption><p><span style="color:green">Preprocessed</span> vs <span style="color:red">Original</span> Images from the testing set; lower is better for each metric; dashed lines represent first degree approximations using least squares regression for the ease of interpretation</p>
</figcaption>
</figure>

:ET