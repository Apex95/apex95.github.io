I";><p>In this work I took a look at Tesseract 4’s performance at recognizing characters from a challenging dataset and proposed a minimalistic convolution-based approach for input image preprocessing that can boost the character-level <strong>accuracy</strong> from <strong>13.4%</strong> to <strong>61.6%</strong> (+359% relative change), and the <strong>F1 score</strong> from <strong>16.3%</strong> to <strong>72.9%</strong> (+347% relative change) on the aforementioned dataset. The convolution kernels are determined using reinforcement learning; moreover, to simulate the lack of ground truth in realistic scenarios, the <strong>training set</strong> consists of only <strong>30</strong> images while the <strong>testing set</strong> includes <strong>10,000</strong>.</p>

<p>The dataset in cause is called <a href="https://pero.fit.vutbr.cz/brno_mobile_ocr_dataset" rel="nofollow">Brno Mobile</a>, and contains colored photographs of typed text, taken with handheld devices. Factors such as blurriness, low resolution, contrast, brightness are contributing to making the images challenging for an OCR engine.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/dataset-sample.png" alt="Resized image from the Brno dataset which contains text that was not recognized by Tesseract 4 during the evaluation (an empty string was returned)" />
  <figcaption><p>Resized image from the Brno dataset which contains text that was not recognized by Tesseract 4 during the evaluation (an empty string was returned)</p>
</figcaption>
</figure>

<p>During this experiment, the <em>out of the box</em> version of Tesseract 4 has been used, which implies:</p>
<ul>
  <li>no retraining of the OCR engine</li>
  <li>no lexicon / dictionary augmentations</li>
  <li>no hints about the language used in the dataset</li>
  <li>no hints about segmentation methods; default (automatic) segmentation is used</li>
</ul>

<h2 id="problem-analysis">Problem Analysis</h2>

<p>Tesseract has proven great performance when tested on favorable datasets by achieving good balance between precision and recall. It is presumed that this evaluation is performed on images that resemble scanned documents or book pages (with or without additional preprocessing) in which the number of camera-caused distortions is minimal. Tests on the Brno dataset led to much worse performance that will be discussed later in the article.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/tesseract-stats.png" alt="Tesseract 4's result when evaluated using the Google Books Dataset - taken from [DAS 2016](https://github.com/tesseract-ocr/docs/tree/master/das_tutorial2016){:rel='nofollow'}" />
  <figcaption><p>Tesseract 4’s result when evaluated using the Google Books Dataset - taken from <a href="https://github.com/tesseract-ocr/docs/tree/master/das_tutorial2016" rel="nofollow">DAS 2016</a></p>
</figcaption>
</figure>

<p>In the above figure, a high precision indicates favorable <em>True-Positives</em> to <em>False-Positives</em> ratio thus revealing proper differentiation between characters (i.e. a relatively small number of misclassifications).</p>

<blockquote>
  <p>“Despite being designed over 20 years ago, the current Tesseract classifier is incredibly difficult to beat with so-called modern methods.” - Ray Smith, author of Tesseract</p>
</blockquote>

<p>I assume that that further training for different fonts might not provide significant improvements and neither will a different model of classifier.</p>

<p>However, it was pointed out in a previous article that <a href="https://codingvision.net/ai/evaluating-the-robustness-of-ocr-systems">Tesseract is not robust to noise</a>; certain <em>salt-and-pepper</em> noise patterns disrupt the character recognition process, leading to large segments of text being completely ignored by the OCR engine - the infamous <strong>empty string</strong>. From empirical observations, these errors seem to occur either for a whole word or sentence or not at all thus suggesting a weakness in the segmentation methodology.</p>

<p>The existence of similar behavior, given images which present more natural distortions, is questioned - hence this experiment.</p>

<h2 id="black-box-considerations">Black-box Considerations</h2>

<p>Since analyzing Tesseract’s segmentation methods is a daunting task, I opted for an adaptive external image correction method. To avoid diving into Tesseract 4’s source code, the OCR engine is considered a black-box; in this case, an unsupervised learning method must be employed. This ensures easier transitions to other OCR engines as it doesn’t directly rely on concrete implementations but only on outputs - at the cost of processing power and optimality.</p>

<h2 id="proposed-solution">Proposed Solution</h2>
<p>The solution consists in directly preprocessing images before they are fed to Tesseract 4. An adaptive preprocessing operation is required, in order to properly compensate for any image features that cause problems in the segmentation process. In other words, an input image must be adapted so it complies with Tesseract 4’s preferences and maximizes the chance of producing the correct output, preferably without performing down-sampling.</p>

<p>I choose a convolution-based approach for flexibility and speed; other articles tend to perform more rigid image adjustments (such as global changes in contrast or brightness, fixed-constant conversion to grayscale, histogram equalization, etc.). I preferred an approach that can properly learn to highlight or mask regions of the image according to various features. For this, the kernels are optimized using reinforcement learning using an actor-critic model. To be more specific, it relies on <em>Twin Delayed Deep Deterministic Policy Gradient</em> (<strong>TD3</strong> for short), for discovering features which minimize the <em>Levenshtein distance</em> between the <strong>recognized text</strong> and the <strong>ground truth</strong>. I’ll not dive into implementation details of TD3 here as it would be somehow out of scope but think of it as a method of optimizing the following formula:</p>

\[\max_{K1,K2,K3,K4,K5}\sum_{i=1}^{N}{-Levenshtein(OCR(Image_i * K1 * K2 * K3 * K4 * K5),Text_i)}\]

<p>Where \(K_j\) is a kernel, and \(&lt;Image_i, Text_i&gt;\) is a tuple from the training set.</p>

<h5 id="a-short-simpler-proof-of-concept-of-the-convolutional-preprocessor-is-presented-in-this-google-colab-it-uses-a-different-architecture-than-the-final-one-and-has-the-purpose-of-verifying-if-the-idea-of-using-convolutions-is-feasible-and-offers-good-results-a-comparison-is-presented-between-original-and-preprocessed-images-thus-revealing-tesseract-4s-segmentation-issue">A short (simpler) proof of concept of the convolutional preprocessor is presented in <a href="https://colab.research.google.com/drive/1l0qT2S3tkY4WHTRbkVK_J5jATPg0t41-?usp=sharing" rel="nofollow">this Google Colab</a>. It uses a different architecture than the final one and has the purpose of verifying if the idea of using convolutions is feasible and offers good results. A comparison is presented between original and preprocessed images, thus revealing Tesseract 4’s segmentation issue.</h5>

<p>The final model is illustrated below, with <strong>ReLU</strong> activations in-between each convolution step to capture nonlinearities and prevent negative values as pixels’ colors.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/convolutional-preprocessor.png" alt="Architecture of the Convolutional Preprocessor used to adapt images for Tesseract 4" />
  <figcaption><p>Architecture of the Convolutional Preprocessor used to adapt images for Tesseract 4</p>
</figcaption>
</figure>

<p>To properly compensate for image coloring and reduce the number of channels (<span style="color:red">R</span>, <span style="color:green">G</span>, <span style="color:blue">B</span>), 1x1 convolutions are used. This prevents overfitting up to a point while also ensuring grayscale output. Further convolutions are applied only on the grayscale image.</p>

<p><em>Symmetry constraints</em> are additionally enforced for each 3x3 kernel in order to minimize the number of trainable parameters and avoid overfitting. This means that for a 3x3 kernel only 6 variables out of 9 must be determined while the rest can be generated through <em>mirroring</em>. Below are the values I got for the five kernels (bold to emphasize symmetry):</p>

<table class="data-table">
  <thead>
    <tr>
      <th>#1</th>
      <th>#2</th>
      <th> </th>
      <th> </th>
      <th>#3</th>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><span style="color:red"><strong>0.7</strong></span></td>
      <td>0.2573</td>
      <td>-0.3</td>
      <td>0.3</td>
      <td>0.3</td>
      <td><strong>-0.2996</strong></td>
      <td>0.3</td>
    </tr>
    <tr>
      <td><span style="color:green"><strong>1.3</strong></span></td>
      <td><strong>0.3</strong></td>
      <td><strong>1.3</strong></td>
      <td><strong>-0.295</strong></td>
      <td>0.3</td>
      <td><strong>1.2949</strong></td>
      <td>0.3</td>
    </tr>
    <tr>
      <td><span style="color:blue"><strong>1.3</strong></span></td>
      <td>0.2573</td>
      <td>-0.3</td>
      <td>0.3</td>
      <td>-0.2802</td>
      <td><strong>0.2922</strong></td>
      <td>-0.2802</td>
    </tr>
  </tbody>
</table>

<table class="data-table">
  <thead>
    <tr>
      <th>#4</th>
      <th> </th>
      <th> </th>
      <th>#5</th>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>-0.2793</strong></td>
      <td>0.2395</td>
      <td>0.2885</td>
      <td>-0.294</td>
      <td>-0.2905</td>
      <td><strong>-0.2939</strong></td>
    </tr>
    <tr>
      <td>0.2395</td>
      <td><strong>0.7119</strong></td>
      <td>0.3</td>
      <td>0.3</td>
      <td><strong>1.162</strong></td>
      <td>-0.2905</td>
    </tr>
    <tr>
      <td>0.2885</td>
      <td>0.3</td>
      <td><strong>-0.2828</strong></td>
      <td><strong>-0.2328</strong></td>
      <td>0.3</td>
      <td>-0.294</td>
    </tr>
  </tbody>
</table>

<h2 id="preprocessing-results">Preprocessing Results</h2>

<p>I extracted the image from each convolution layer and clamped its values to the <em>0-255</em> interval to properly view each transformation:</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/transformations.png" alt="Transformations of an image as it passes through the convolutional preprocessor, viewed from left (original) to right (final sample); observe the removal of incomplete characters from the upper-left region" />
  <figcaption><p>Transformations of an image as it passes through the convolutional preprocessor, viewed from left (original) to right (final sample); observe the removal of incomplete characters from the upper-left region</p>
</figcaption>
</figure>

<h2 id="comparison">Comparison</h2>

<p>I used 10,000 images from the testing set for the evaluation of the current methodology and compiled the following graphs. The differences between original and preprocessed samples are illustrated with three metrics of interest: <em>Character Error Rate</em> (<strong>CER</strong>), <em>Word Error Rate</em> (<strong>WER</strong>) and <em>Longest Common Subsequence Error</em> (<strong>LCSE</strong>). <strong>LCSE</strong> is computed as follows:</p>

\[LCSE(Text_1,Text_2 )=|Text_1 |-|LCS(Text_1,Text_2 )|+|Text_2 |-|LCS(Text_1,Text_2 )|\]

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/results-comparison.png" alt="&lt;span style='color:green'&gt;Preprocessed&lt;/span&gt; vs &lt;span style='color:red'&gt;Original&lt;/span&gt; Images from the testing set; lower is better for each metric; dashed lines represent first degree approximations using least squares regression for the ease of interpretation" />
  <figcaption><p><span style="color:green">Preprocessed</span> vs <span style="color:red">Original</span> Images from the testing set; lower is better for each metric; dashed lines represent first degree approximations using least squares regression for the ease of interpretation</p>
</figcaption>
</figure>

<p>Additionally, I plotted everything in histogram format to properly see the distributions of errors. For <strong>CER</strong> and <strong>WER</strong>, it is easy to observe the spikes around <strong>1</strong> (100%) that suggest the aforementioned segmentation problem (at block-of-text level) is the most frequent error (<strong>empty strings</strong> are returned). In certain situations, the <strong>WER</strong> is larger than <strong>1</strong> because the preprocessing step introduces artifacts near the border of the image thus leading to recognition of non-existent characters. When looking at the <strong>LCSE</strong> plot, a distribution shift can be seen from the original approximately gaussian shape with its peak (mode) near the average number of characters in an image (<strong>56.95</strong>) to a more favorable shape with overall lower error rates.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/results-distributions.webp" alt="&lt;span style='color:green'&gt;Preprocessed&lt;/span&gt; vs &lt;span style='color:red'&gt;Original&lt;/span&gt; Images from the testing set; comparison of distributions of errors" />
  <figcaption><p><span style="color:green">Preprocessed</span> vs <span style="color:red">Original</span> Images from the testing set; comparison of distributions of errors</p>
</figcaption>
</figure>

<p>A numeric comparison is presented below:</p>

<table class="data-table">
  <thead>
    <tr>
      <th>Metric</th>
      <th>Original (Avg.)</th>
      <th>Preprocessed (Avg.)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CER</td>
      <td>0.866</td>
      <td>0.384</td>
    </tr>
    <tr>
      <td>WER</td>
      <td>0.903</td>
      <td>0.593</td>
    </tr>
    <tr>
      <td>LCSE</td>
      <td>48.834</td>
      <td>24.987</td>
    </tr>
    <tr>
      <td>Precision</td>
      <td>0.155</td>
      <td>0.725</td>
    </tr>
    <tr>
      <td>Recall</td>
      <td>0.172</td>
      <td>0.734</td>
    </tr>
    <tr>
      <td>F1 Score</td>
      <td>0.163</td>
      <td>0.729</td>
    </tr>
  </tbody>
</table>

<h2 id="takeaways">Takeaways</h2>

<p>Significant improvements can be observed through this preprocessing operation. Moreover, the majority of errors probably do not occur in the <em>sequence to sequence</em> classifier (since all the recognized characters are erroneous and would contradict previous performance analysis) and would most likely be a page-segmentation issue, when automatic mode is used. It is shown that an array of convolutions is sufficient to decrease error rates substantially.</p>

<p>The OCR performance on the preprocessed images is overall better but not good enough to be reliable. A 38% character error rate is still a large setback. I’m pretty sure that better recognitions can be obtained with more fine-tuning and a different architecture for the convolutional preprocessor. In this article I wanted something relatively simple and that runs fast.</p>

:ET