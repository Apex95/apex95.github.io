I"pE<p>In this work I took a look at Tesseract 4’s performance at recognizing characters from a challenging dataset and proposed a minimalistic convolution-based approach for input image preprocessing that can boost the character-level <strong>accuracy</strong> from <strong>13.4%</strong> to <strong>61.6%</strong> (+359% relative change), and the <strong>F1 score</strong> from <strong>16.3%</strong> to <strong>72.9%</strong> (+347% relative change) on the aforementioned dataset. The convolution kernels are determined using reinforcement learning; moreover, to simulate the lack of ground truth in realistic scenarios, the <strong>training set</strong> consists of only <strong>30</strong> images while the <strong>testing set</strong> includes <strong>10,000</strong>.</p>

<p>The dataset in cause is called <a href="https://pero.fit.vutbr.cz/brno_mobile_ocr_dataset" rel="nofollow">Brno</a>, and contains colored photographs of typed text, taken with handheld devices. Factors such as blurriness, low resolution, contrast, brightness are contributing to making the images challenging for an OCR engine.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/dataset-sample.png" alt="Resized image from the Brno dataset which contains text that was not recognized by Tesseract 4 during the evaluation (an empty string was returned)" />
  <figcaption><p>Resized image from the Brno dataset which contains text that was not recognized by Tesseract 4 during the evaluation (an empty string was returned)</p>
</figcaption>
</figure>

<p>During this experiment, the <em>out of the box</em> version of Tesseract 4 has been used, which implies:</p>
<ul>
  <li>no retraining of the OCR engine</li>
  <li>no lexicon / dictionary augmentations</li>
  <li>no hints about the language used in the dataset</li>
  <li>no hints about segmentation methods; default (automatic) segmentation is used</li>
</ul>

<h2 id="problem-analysis">Problem Analysis</h2>

<h4 id="despite-being-designed-over-20-years-ago-the-current-tesseract-classifier-is-incredibly-difficult-to-beat-with-so-called-modern-methods"><em>“Despite being designed over 20 years ago, the current Tesseract classifier is incredibly difficult to beat with so-called modern methods.”</em></h4>

<p>Tesseract 4 has proven great performance when tested on favorable datasets by achieving good balance between precision and recall. It is presumed that this evaluation is performed on images that resemble scanned documents or book pages (with or without additional preprocessing) in which the number of distortions is minimal. Tests on the Brno dataset led to much worse performance that will be discussed later in the article.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/tesseract-stats.png" alt="Tesseract 4's result when evaluated using the Google Books Dataset - taken from [DAS 2016](https://github.com/tesseract-ocr/docs/tree/master/das_tutorial2016){:rel='nofollow'}" />
  <figcaption><p>Tesseract 4’s result when evaluated using the Google Books Dataset - taken from <a href="https://github.com/tesseract-ocr/docs/tree/master/das_tutorial2016" rel="nofollow">DAS 2016</a></p>
</figcaption>
</figure>

<p>A high precision indicates favorable <em>True-Positives</em> to <em>False-Positives</em> ratio thus revealing proper differentiation between characters (i.e. a relatively small number of misclassifications). I assume that that further training for different fonts might not provide significant improvements.</p>

<p>However, it was pointed out in a previous article that <a href="https://codingvision.net/ai/evaluating-the-robustness-of-ocr-systems">Tesseract is not robust to noise</a>; certain <em>salt-and-pepper</em> noise patterns disrupt the character recognition process, leading to large segments of text being completely ignored by the OCR engine. This suggests a weakness in the segmentation methodology.</p>

<p>The existence of similar behavior, given images which present more natural distortions, is questioned - hence the experiment.</p>

<h2 id="constraints">Constraints</h2>

<p>Tesseract 4 seems to rely on machine learning to perform <em>sequence to sequence</em> recognition using <em>Connectionist Temporal Classification</em> (<strong>CTC</strong>) as a loss function. Rows of text are fed directly into a <em>Convolutional Recurrent Neural Network</em> (<strong>CRNN</strong>) which analyzes portions of image and outputs probabilities that are decoded into recognized text using <em>beam search</em>.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/improving-tesseract-4-ocr-accuracy-through-image-preprocessing/tesseract-lstm.png" alt="Tesseract 4's architecture as presented in [Modernization Efforts - DAS 2016](https://github.com/tesseract-ocr/docs/blob/master/das_tutorial2016/6ModernizationEfforts.pdf){:rel='nofollow'}" />
  <figcaption><p>Tesseract 4’s architecture as presented in <a href="https://github.com/tesseract-ocr/docs/blob/master/das_tutorial2016/6ModernizationEfforts.pdf" rel="nofollow">Modernization Efforts - DAS 2016</a></p>
</figcaption>
</figure>

<p>This approach makes the presented issue difficult to debug as it most probably takes into account some uncanny features when performing classifications. A direct consequence is that the model might treat image distortions as features and start outputting <em>blanks</em> (-) thus leading to the infamous empty strings returned by Tesseract. Discovering these features is a daunting task, therefore external image correction is considered. Moreover, to avoid diving into Tesseract 4’s source code and compute gradients, the OCR engine is considered a black-box; a direct consequence is that the image corrections might not have the same quality as if gradient-based optimization was used but will allow for easier embedding with different OCR software.</p>

<h2 id="proposed-solution">Proposed Solution</h2>
<p>The solution consists in directly preprocessing images before they are fed to Tesseract 4. An adaptive preprocessing operation is required, in order to properly compensate for any image features that cause problems in the recognition mechanism. In other words, an input image must be adapted to it complies to Tesseract 4’s preferences and maximizes the chance of producing the correct output.</p>

<h5 id="a-short-proof-of-concept-is-presented-in-this-google-colab-it-uses-a-different-architecture-than-the-final-one-and-has-the-purpose-of-verifying-if-the-idea-of-using-convolutions-is-feasible-and-offers-good-results-a-comparison-is-presented-between-original-and-preprocessed-images-thus-revealing-tesseract-4s-issue">A short proof of concept is presented in <a href="https://colab.research.google.com/drive/1l0qT2S3tkY4WHTRbkVK_J5jATPg0t41-?usp=sharing">this Google Colab</a>. It uses a different architecture than the final one and has the purpose of verifying if the idea of using convolutions is feasible and offers good results. A comparison is presented between original and preprocessed images, thus revealing Tesseract 4’s issue.</h5>

<h3 id="5-dictionaries">5. Dictionaries</h3>

<p>OCR systems will attempt to improve their accuracy by employing dictionaries with predefined words. Altering a single character in a word (i.e.: the incremental approach) might not be effective in this case.</p>

<h2 id="targeted-ocr-systems">Targeted OCR Systems</h2>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/tesseract-gocr.png" alt="Tested locally on Tesseract 4 and remotely on Google's Cloud Vision OCR" />
  <figcaption><p>Tested locally on Tesseract 4 and remotely on Google’s Cloud Vision OCR</p>
</figcaption>
</figure>

<p>For this project, I used <strong>Tesseract 4</strong> for prototyping and testing, as it had no timing restrictions and allowed me to run a fast, parallel model with high throughput so I could test if the implementation works as expected. Later, I moved to <strong>Google’s Cloud Vision OCR</strong> and tried some ‘remote’ fuzzing through the API.</p>

<h2 id="methodology">Methodology</h2>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/architecture.png" alt="A rather simplified view of the flow; a feedback-based adversarial samples generator (in image: obfuscator) alters inputs in order to maximize the error of the OCR system" />
  <figcaption><p>A rather simplified view of the flow; a feedback-based adversarial samples generator (in image: obfuscator) alters inputs in order to maximize the error of the OCR system</p>
</figcaption>
</figure>

<p>In order to be able to cover even black box cases, I used a <strong>genetic algorithm</strong> guided by the feedback of the targeted OCR system. We observe that the confidence of the classifier, alone, is not a good metric for this problem, a score function based on the <a href="https://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">Levenshtein distance</a> and the <strong>amount of noise</strong> is employed.</p>

<p>One of the main problems here was the size of the search space which was partially solved by identifying regions of interest in the image and focusing only on these. Also, lots of parameter tuning…</p>

<h2 id="noise-properties">Noise properties</h2>

<p>Given the constraints, the following properties of the noise model must be matched:</p>

<ul>
  <li><strong>high contrast</strong> – so it bypasses the binarization process and generates feedback</li>
  <li><strong>low density</strong> – in order to maintain readability by exploiting the natural <strong>low-filtering</strong> capability of the human vision</li>
</ul>

<p>Applying <strong>salt-and-pepper</strong> noise in a smart manner will, hopefully, satisfy the constraints.</p>

<h2 id="working-modes">Working modes</h2>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/modes.png" alt="Different working modes for small and large characters, in order to preserve readability. Both managed to entirely hide the given text when tested on Tesseract 4" />
  <figcaption><p>Different working modes for small and large characters, in order to preserve readability. Both managed to entirely hide the given text when tested on Tesseract 4</p>
</figcaption>
</figure>

<p>Initially, the algorithm worked using only <strong>overtext</strong> mode, which applied noise in the rectangle which contained characters. However, this method is not the best choice for texts written using smaller characters mainly because there are less pixels that can be altered thus drastically lowering the readability even with minimal amounts of noise. For this special case, the decision to insert the noise in-between the text rows (<strong>artifacts</strong>) was taken in order to preserve the original characters. Both methods presented similar success rates in hiding texts from the targeted OCR system.</p>

<p>Just for fun, here’s what happens if the score function is inverted, which translates as “generate an image with as much noise as possible, but which can be read by OCR software”. Weird, but it’s still recognized…</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/inverted-function.png" alt="Tesseract recognized the original text with **no errors**. How about you?" />
  <figcaption><p>Tesseract recognized the original text with <strong>no errors</strong>. How about you?</p>
</figcaption>
</figure>

<h2 id="results-on-tesseract">Results on Tesseract</h2>

<p>Promising results were achieved while testing against Tesseract 4. In the following figure is presented an early (non-final) sample in which the word “<strong>Random</strong>” is not recognized by Tesseract:</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/tess-results-ui.png" alt="The first word is successfully hidden from the OCR system" />
  <figcaption><p>The first word is successfully hidden from the OCR system</p>
</figcaption>
</figure>

<h2 id="tests-on-googles-cloud-vision-platform">Tests on Google’s Cloud Vision Platform</h2>

<p>This is where things get interesting.</p>

<h5 id="the-implemented-score-function-can-be-maximized-in-2-ways-hiding-characters-or-tricking-the-ocr-engine-into-adding-characters-which-shouldnt-be-there">The implemented score function can be maximized in 2 ways: hiding characters or tricking the OCR engine into adding characters which shouldn’t be there.</h5>

<p>One of the samples managed to create a <strong>loop</strong> in the recognition process of <strong>Google’s Cloud Vision OCR</strong>, basically recognizing the same text multiple times. No <strong>DoS</strong> or anything (or I’m not aware of it), I’m still not sure if the loop persisted or not - it either produced a small number of iterations, failed (timed out?) or they had load balancers which compensated for this and used different instances.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/cloud_ocr_bug.png" alt="Possible loop in the recognition process: the same text gets recognized multiple times. The bottom-left and the top-right corners are 'merged' into an oblique text row so the recognition process is sent back to already processed text." />
  <figcaption><p>Possible loop in the recognition process: the same text gets recognized multiple times. The bottom-left and the top-right corners are ‘merged’ into an oblique text row so the recognition process is sent back to already processed text.</p>
</figcaption>
</figure>

<p>Let’s take a closer look at the sample: below, you can see how the adversarial sample was interpreted by Google’s Cloud Vision OCR system. The image was submitted directly to the Cloud Vision platform via the <a href="https://cloud.google.com/vision/" rel="nofollow">“Try the API”</a> option so, at the moment of testing, the results could be easily reproduced.</p>
<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/cloud_ocr_bug2.png" alt="Rectangles returned by Cloud Vision indicate that additional text rows are 'created' during the recognition thus creating a loop" />
  <figcaption><p>Rectangles returned by Cloud Vision indicate that additional text rows are ‘created’ during the recognition thus creating a loop</p>
</figcaption>
</figure>

<p>Also the ‘boring’ case where the characters are hidden:</p>
<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/cloud-ocr-artifacts.png" alt="Once again, using the artifacts mode on a small text since larger texts are way easier to hide" />
  <figcaption><p>Once again, using the artifacts mode on a small text since larger texts are way easier to hide</p>
</figcaption>
</figure>

<h2 id="conclusions">Conclusions</h2>

<p>It works, but the project reached its objective and is no longer in development.
It seems difficult to create samples that work for all OCR systems (<strong>generalization</strong>).</p>

<p>Also, the samples are vulnerable to changes at the <strong>preprocessing</strong> stage in the OCR pipeline such as:</p>

<ul>
  <li>noise filtering (e.g.: median filters)</li>
  <li>compression techniques (e.g.: Fourier compression)</li>
  <li>downscaling-&gt;upscaling (e.g.: Autoencoders)</li>
</ul>

<p>However, we can conclude that, using this approach, it is more challenging to mask small characters without making the text difficult to read. I compiled the following graph, in which are compared: the images generated by the algorithm (below <strong>7%</strong> noise density) and a set of images that contain random noise (<strong>15%</strong> noise density). The 2 sets contain different images with characters of sizes: 12, 21, 36, 50. Each random noise set contains 62 samples for each size - average values were used.</p>

<p><strong>Noise efficiency</strong> is computed by taking into account the <strong>Levenshtein distance</strong> and the total <strong>amount of noise</strong> in the image.</p>

<figure class="image">
  <img src="data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=" data-echo="/imgs/posts/evaluating-the-robustness-of-ocr-systems/noise-eff-cloudocr.png" alt="As characters get smaller, the efficiency of the noise added by the algorithm decreases - the random noise samples behave in an opposite manner." />
  <figcaption><p>As characters get smaller, the efficiency of the noise added by the algorithm decreases - the random noise samples behave in an opposite manner.</p>
</figcaption>
</figure>

<h2 id="interesting-todos">Interesting TODO’s</h2>

<ul>
  <li>Extracting templates from samples and training a generator?</li>
  <li>Exploiting directly the row segmentation feature?</li>
  <li>Attacking Otsu’s binarization method?</li>
</ul>

<p>Maybe someday…</p>

:ET